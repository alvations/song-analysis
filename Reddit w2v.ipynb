{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import multiprocessing\n",
    "import os, re, sys\n",
    "import nltk\n",
    "import csv\n",
    "import gensim.models.word2vec as w2v\n",
    "import glob2\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# word2vec with Reddit corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "songdata_filepath = \"songdata.csv\"                #simple csv\n",
    "\n",
    "reddit_filepath = \"reddit-top-2.5-million-master/data\" #this is a dir, we'll walk through it to create a\n",
    "                                                  #dict of subreddits and posts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## just playing arond with etl on the song data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "songs_list = []\n",
    "\n",
    "with open(songdata_filepath, 'rb') as csvfile:\n",
    "    song_reader = csv.reader(csvfile)\n",
    "    i = 0\n",
    "    for row in song_reader:\n",
    "        if(i == 0):\n",
    "            next\n",
    "        else:\n",
    "            song = {'artist': row[0], 'title': row[1], 'lyric_link': row[2], 'text': row[3]}\n",
    "            songs_list.append(song)\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'artist': 'Air Supply',\n",
       " 'lyric_link': '/a/air+supply/do+it+again_20004902.html',\n",
       " 'text': \"I used to watch you walk away  \\nTil the road you take would bend  \\nI was the man to turn you on  \\nAnd you knew it after all  \\nCome tomorrow when I'm all weary  \\nYou would turn me down again  \\nAt least that's the way I can remember  \\n  \\nDo it again, do it again  \\nDo it again, do it again  \\n  \\nEvery little word you told me got home  \\nI'm still thinking about them  \\nEvery little secret we could have known  \\nAnd we can't live without them  \\n  \\nYou surely started something  \\nYou surely gave me something new  \\n  \\nDo it again, do it again  \\nDo it again, do it again  \\n  \\nAnd I do think about you each now and then  \\nAnd each time it gets tighter  \\nFrom the road I can see the last break  \\nAnd my head's getting lighter  \\n  \\nDo it again, do it again (repeat)  \\n\\n\",\n",
       " 'title': 'Do It Again'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "songs_list[258]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57650"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(songs_list)   #should be 57,650 songs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now on to the subreddit posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit_list = {}\n",
    "\n",
    "path = reddit_filepath + '/**'\n",
    "files = glob2.glob(path)\n",
    "\n",
    "len(files)   #should be 2,499 subreddits\n",
    "\n",
    "subreddit_id_to_title = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 0\n",
    "for file in files:\n",
    "    subreddit = file.replace(reddit_filepath + \"/\", \"\").replace(\".csv\", \"\")\n",
    "    subreddit_id_to_title[j] = subreddit\n",
    "    j += 1\n",
    "    with(open(file, 'rb')) as csvfile:\n",
    "        reddit_reader = csv.reader(csvfile)\n",
    "        posts = []\n",
    "        reddit_reader.next()\n",
    "        for post in reddit_reader:\n",
    "            posts.append({'score': post[1].decode('utf-8'), 'title': post[4].decode('utf-8'), 'text': post[9].decode('utf-8')})\n",
    "        \n",
    "        subreddit_list[subreddit] = posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': u'430',\n",
       " 'text': u\"If you saw my tweet yesterday about being single now, that's the reason I haven't had that much inspiration lately to work on a game as big as 0x10c, and hopefully it explains why I haven't spoken much about the reason.\\n\\nI don't want to sound whiny on twitter, but I thought this excellent subreddit might want to know. I haven't forgotten about you!\",\n",
       " 'title': u'The reason for slow'}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subreddit_list[\"0x10c\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_corpus = \"\"\n",
    "offset = 2495                                             #testing param only. this leaves you with the first 5 \n",
    "                                                          #subreddits, with 1000 posts each for 5k posts\n",
    "for i in range(0, len(subreddit_list) - 1 - offset):\n",
    "    \n",
    "    subreddit = subreddit_list[subreddit_id_to_title[i]]\n",
    "    for post in subreddit:\n",
    "        string_ripped_out = post['title'] + \" \" + post['text']\n",
    "        reddit_corpus += string_ripped_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "raw_sentences = tokenizer.tokenize(reddit_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263163\n"
     ]
    }
   ],
   "source": [
    "def sentence_to_wordlist(raw):\n",
    "    \"\"\"\n",
    "    convert to list of words\n",
    "    remove unnecessary, split into words, no hyphens\n",
    "    list of words\n",
    "    \"\"\"\n",
    "    clean = re.sub(\"[^a-zA-z]\", \" \", raw)\n",
    "    words = clean.split()\n",
    "    return words\n",
    "\n",
    "sentences = []\n",
    "for raw_sentence in raw_sentences:\n",
    "    if len(raw_sentence) > 0:\n",
    "        sentences.append(sentence_to_wordlist(raw_sentence))\n",
    "\n",
    "token_count = sum([len(sentence) for sentence in sentences])\n",
    "print(token_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reddit_vec = w2v.Word2Vec(\n",
    "            sg=1,\n",
    "            seed=1337,\n",
    "            workers=4,\n",
    "            size=300,\n",
    "            min_count=3,\n",
    "            window=7,\n",
    "            sample=0.0001\n",
    "        )\n",
    "\n",
    "reddit_vec.build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1194365"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_vec.train(sentences, total_examples=len(sentences), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'watch', 0.9908463954925537),\n",
       " (u'feature', 0.9521093964576721),\n",
       " (u'page', 0.9492238759994507),\n",
       " (u'search', 0.9221713542938232),\n",
       " (u'result', 0.9204763174057007),\n",
       " (u'[this]', 0.9118918180465698),\n",
       " (u'com', 0.9076484441757202),\n",
       " (u'screenshot', 0.9025120735168457),\n",
       " (u'jpg', 0.9006019830703735),\n",
       " (u'images', 0.899824857711792)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_vec.most_similar(\"youtube\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
