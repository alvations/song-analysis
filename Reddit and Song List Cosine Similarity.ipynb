{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import multiprocessing\n",
    "import os, re, sys\n",
    "import nltk\n",
    "import csv\n",
    "import gensim.models.word2vec as w2v\n",
    "import glob2\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "songdata_filepath = \"songdata.csv\"                #simple csv\n",
    "\n",
    "reddit_filepath = \"reddit-top-2.5-million-master/data\" #this is a dir, we'll walk through it to create a\n",
    "                                                  #dict of subreddits and posts\n",
    "\n",
    "songs_list = []\n",
    "\n",
    "with open(songdata_filepath, 'rb') as csvfile:\n",
    "    song_reader = csv.reader(csvfile)\n",
    "    i = 0\n",
    "    for row in song_reader:\n",
    "        if(i == 0):\n",
    "            next\n",
    "        else:\n",
    "            song = {'artist': row[0], 'title': row[1], 'lyric_link': row[2], 'text': row[3]}\n",
    "            songs_list.append(song)\n",
    "        i += 1\n",
    "        \n",
    "\n",
    "subreddit_list = {}\n",
    "\n",
    "path = reddit_filepath + '/**'\n",
    "files = glob2.glob(path)\n",
    "\n",
    "len(files)   #should be 2,499 subreddits\n",
    "\n",
    "subreddit_id_to_title = {}\n",
    "\n",
    "j = 0\n",
    "for file in files:\n",
    "    subreddit = file.replace(reddit_filepath + \"/\", \"\").replace(\".csv\", \"\")\n",
    "    subreddit_id_to_title[j] = subreddit\n",
    "    j += 1\n",
    "    with(open(file, 'rb')) as csvfile:\n",
    "        reddit_reader = csv.reader(csvfile)\n",
    "        posts = []\n",
    "        reddit_reader.next()\n",
    "        for post in reddit_reader:\n",
    "            #posts.append({'score': post[1].decode('utf-8'), 'title': post[4].decode('utf-8'), 'text': post[9].decode('utf-8')})\n",
    "            posts.append({'score': post[1], 'title': post[4], 'text': post[9]})\n",
    "        subreddit_list[subreddit] = posts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(str):\n",
    "    # remove links\n",
    "    #str = re.sub(r'http(s)?:\\/\\/\\S*? ', \"\", str)\n",
    "    str = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', \"\", str)\n",
    "    return str\n",
    "\n",
    "subreddits_without_titles = []\n",
    "offset = 2400\n",
    "\n",
    "for i in range(0, len(subreddit_list) - 1 - offset):\n",
    "    reddit_corpus = \"\"\n",
    "    subreddit = subreddit_list[subreddit_id_to_title[i]]\n",
    "    for post in subreddit:\n",
    "        string_ripped_out = post['title'] + \" \" + post['text']\n",
    "        reddit_corpus += preprocess(string_ripped_out)\n",
    "        \n",
    "    subreddits_without_titles.append(reddit_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs_without_titles = []\n",
    "for song in songs_list:\n",
    "    lyrics = song['text']\n",
    "    songs_without_titles.append(lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_docs = [subreddits_without_titles[10]] + songs_without_titles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_matrix = tfidf_vectorizer.fit_transform(full_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.          0.03148522  0.00885396 ...,  0.03552177  0.01527526\n",
      "  0.03386445]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'artist': 'Alphaville',\n",
       " 'lyric_link': '/a/alphaville/all+in+the+golden+afternoon_20006851.html',\n",
       " 'text': \"All in the golden afternoon full leisurely we glide  \\nFor both our oars, with little skill, by little arms are plied  \\nWhile little hands make vain pretence our wanderings to guide  \\nAh, cruel three! in such an hour beneath such dreamy wheather  \\nTo beg a tale of breath too weak to stir the tiniest feather  \\nAnd what can one poor voice avail against three tongues together  \\nAnon, to sudden silence won, in fancy they pursue  \\nThe dream child moving through a land of wonders wild and new  \\n  \\nIn friendly chat with bird or beast- and half believe it true  \\nAnd ever as the story drained the wells of fancy dry and faintly strove that\\nweary one to put the subject by  \\nThe next time. it is next time the happy voices cry  \\nThus grew the tale of wonderland, thus slowly, one by one  \\nIt's quaint events were hammered out  \\nAnd now the tale is done and home we steer  \\nA merry crew  \\nBeneath the setting sun\\n\\n\",\n",
       " 'title': 'All In The Golden Afternoon'}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(result[0])\n",
    "\n",
    "temp = result[0].tolist()\n",
    "temp.remove(max(result[0]))\n",
    "\n",
    "songs_list[temp.index(max(temp))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          0.03148522  0.00885396 ...,  0.03552177  0.01527526\n",
      "   0.03386445]]\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
